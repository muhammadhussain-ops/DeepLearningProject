{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "path = r'X:\\Processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "# Define the number of features (i.e., columns in your dataset)\n",
    "#num_features = 11*window_size\n",
    "\n",
    "num_features = 330\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units1, hidden_units2, hidden_units3, latent_features):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=hidden_units1, kernel_size=3, stride=2, padding=1),  # Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units1, hidden_units2, kernel_size=3, stride=2, padding=1),              # Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units2, hidden_units3, kernel_size=3, stride=2, padding=1),              # Layer 3\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_units3 * 4 * 2, latent_features)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_features, hidden_units3 * 4 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (hidden_units3, 4, 2)),\n",
    "            nn.Conv2d(hidden_units3, hidden_units2, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units2, hidden_units1, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units1, 1, kernel_size=3, stride=2, padding=1),  \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return {'z': z, 'x_hat': x_hat}\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "hidden_units1 = 256\n",
    "hidden_units2 = 128\n",
    "hidden_units3 = 64\n",
    "latent_features = 32\n",
    "net = AutoEncoder(hidden_units1, hidden_units2, hidden_units3, latent_features)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only put batches onto gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "class FragmentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # Data is already a GPU tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = x.view(1, 30, 11)  # Reshape to (1, 30, 11)\n",
    "        return x, x  # Input and target for autoencoder\n",
    "\n",
    "# Directory where the separate Parquet files are stored\n",
    "fragments_dir = r'X:\\Processed_data'\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "model = AutoEncoder(hidden_units1, hidden_units2, latent_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)  # Move model to the device\n",
    "\n",
    "# Iterate through the fragment files\n",
    "fragment_files = [os.path.join(fragments_dir, f) for f in os.listdir(fragments_dir) if f.endswith('.parquet')]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_loss = []\n",
    "\n",
    "    for file_path in fragment_files:\n",
    "        print(f\"Processing fragment: {file_path}\")\n",
    "\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        data = parquet_file.read().to_pandas().to_numpy(dtype='float32')\n",
    "\n",
    "        fragment_dataset = FragmentDataset(data)\n",
    "        train_loader = DataLoader(fragment_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for x, _ in train_loader:\n",
    "            # x is of shape (batch_size, 1, 30, 11)\n",
    "            outputs = model(x)\n",
    "            x_hat = outputs['x_hat']\n",
    "            loss = loss_function(x_hat, x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"autoencoder_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all on gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FragmentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # Data is already a GPU tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        x = x.view(1, 30, 11)  # Original shape: (1, 30, 11)\n",
    "        x = F.pad(x, pad=(0, 5, 0, 2))  # Pad (left, right, top, bottom) to reach (1, 32, 16)\n",
    "        return x, x  # Input and target for autoencoder\n",
    "\n",
    "# Directory where the separate Parquet files are stored\n",
    "fragments_dir = r'X:\\Processed_data'\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "num_epochs = 2\n",
    "batch_size = 512  # Adjust based on GPU memory\n",
    "model = AutoEncoder(hidden_units1, hidden_units2, hidden_units3, latent_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)  # Move model to the device\n",
    "\n",
    "# Iterate through the fragment files\n",
    "fragment_files = [\n",
    "    os.path.join(fragments_dir, f)\n",
    "    for f in os.listdir(fragments_dir)\n",
    "    if f.endswith('.parquet')\n",
    "]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_loss = []\n",
    "\n",
    "    for file_path in fragment_files:\n",
    "        print(f\"Processing fragment: {file_path}\")\n",
    "\n",
    "        # Load data in main process\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        data = parquet_file.read().to_pandas().to_numpy(dtype='float32')\n",
    "\n",
    "        # Convert data to tensor and move to GPU\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Create Dataset with data already on GPU\n",
    "        fragment_dataset = FragmentDataset(data_tensor)\n",
    "\n",
    "        # Since data is on GPU, no need for multiple workers or pin_memory\n",
    "        train_loader = DataLoader(\n",
    "            fragment_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0  # Must be 0 when data is on GPU\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        for x, _ in train_loader:\n",
    "            # x is of shape (batch_size, 1, 32, 16)\n",
    "            outputs = model(x)\n",
    "            x_hat = outputs['x_hat']\n",
    "            loss = loss_function(x_hat, x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model, f\"autoencoder_CNN_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the entire model\n",
    "net_loaded = torch.load('autoencoder_epoch_2.pth', map_location=device)\n",
    "net_loaded.to(device)\n",
    "net_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the detect_anomalies function with batch processing\n",
    "def detect_anomalies_in_batches(data, model, device, batch_size=512, threshold=0.08):\n",
    "    model.eval()\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(data, dtype=torch.float32))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            data_tensor = batch[0].to(device)\n",
    "            outputs = model(data_tensor)\n",
    "            x_hat = outputs['x_hat']\n",
    "            batch_errors = torch.mean((data_tensor - x_hat) ** 2, dim=1).cpu().numpy()\n",
    "            reconstruction_errors.extend(batch_errors)\n",
    "    \n",
    "    reconstruction_errors = np.array(reconstruction_errors)\n",
    "    anomalies = reconstruction_errors > threshold\n",
    "    return anomalies, reconstruction_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test på træningsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the separate Parquet files are stored\n",
    "fragments_dir = r'X:\\Processed_data'\n",
    "\n",
    "# Iterate through the fragment files\n",
    "fragment_files = [\n",
    "    os.path.join(fragments_dir, f)\n",
    "    for f in os.listdir(fragments_dir)\n",
    "    if f.endswith('.parquet')\n",
    "]\n",
    "\n",
    "file_number = 1\n",
    "\n",
    "# Ensure that device is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "# Assuming 'model' is your trained model\n",
    "# If you have saved the model, load it using:\n",
    "# model.load_state_dict(torch.load('autoencoder_epoch_10.pth'))\n",
    "model = AutoEncoder(hidden_units1, hidden_units2, hidden_units3, latent_features)\n",
    "model.to(device)\n",
    "\n",
    "# Read the data from the Parquet file\n",
    "print(fragment_files[file_number])\n",
    "parquet_file = pq.ParquetFile(fragment_files[file_number])\n",
    "data_table = parquet_file.read()\n",
    "data = data_table.to_pandas().to_numpy(dtype='float32')\n",
    "\n",
    "\n",
    "# Call the detect_anomalies function\n",
    "anomalies, reconstruction_errors = detect_anomalies_in_batches(data, model, device)\n",
    "\n",
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.75)\n",
    "plt.axvline(x=0.08, color='r', linestyle='--')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print out the indices of anomalies\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "print(f\"Detected {len(anomaly_indices):,} anomalies out of {len(data):,} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = pq.read_table(fragment_files[file_number])\n",
    "data_df = data_table.to_pandas()\n",
    "data_df['reconstruction_error'] = reconstruction_errors\n",
    "data_df['anomaly'] = anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test på valideringsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezer_number = \"806031\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "input_file = freezer_number + \"_temp.parquet\"  # Replace with the correct file name if needed\n",
    "\n",
    "# Specify the columns to scale\n",
    "columns_to_scale = [\n",
    "    'RTD', '1st Suc.', 'Cond. Air In', 'Evap. In', 'Evap. Out', \n",
    "    '2nd Suc.', 'Chil. water In', '2nd Sump', 'H.E.', \n",
    "    'SetPoint', 'Mains Voltage'\n",
    "]\n",
    "\n",
    "# Load the combined data (assuming it's already saved as 'combined_data.parquet')\n",
    "data = pd.read_parquet(path + input_file)\n",
    "\n",
    "# Filter out rows where the freezer is off\n",
    "data = data[(data['Mains Voltage'] >= 200)]\n",
    "\n",
    "# Check if all columns are present in the dataset\n",
    "missing_columns = [col for col in columns_to_scale if col not in data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following columns are missing in the data: {missing_columns}\")\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the specified columns\n",
    "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "# Save the scaled dataset back to Parquet\n",
    "scaled_output_file = \"scaled_\" + freezer_number + \"_data.parquet\"\n",
    "data.to_parquet(path + scaled_output_file)\n",
    "\n",
    "print(f\"Scaled data saved as {scaled_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from joblib import Parallel, delayed as joblib_delayed\n",
    "from tqdm import tqdm\n",
    "import gc  # Import garbage collection module\n",
    "\n",
    "##### Brug ML env #####\n",
    "\n",
    "# Constants\n",
    "input_path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "output_path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "window_size = 30\n",
    "num_cores = -1  # Use all available cores\n",
    "\n",
    "# List of freezer numbers to process\n",
    "freezer_numbers = [ freezer_number ]\n",
    "\n",
    "# Function to process a single freezer\n",
    "def process_freezer(freezer_number):\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    temp_output_file = os.path.join(temp_dir, f\"processed_{freezer_number}_temp\")\n",
    "    final_output_file = os.path.join(output_path, f\"test_ready_{freezer_number}.parquet\")\n",
    "\n",
    "    try:\n",
    "        # Load data using Dask\n",
    "        df = dd.read_parquet(input_path + f'scaled_{freezer_number}_data.parquet')\n",
    "        \n",
    "        # Step 1: Drop unnecessary columns\n",
    "        drop_columns_set_1 = ['State', 'Type', 'Event']\n",
    "        drop_columns_set_2 = drop_columns_set_1 + ['main_fault']\n",
    "        if 'main_fault' in df.columns:\n",
    "            df = df.drop(columns=[col for col in drop_columns_set_2 if col in df.columns])\n",
    "        else:\n",
    "            df = df.drop(columns=[col for col in drop_columns_set_1 if col in df.columns])\n",
    "\n",
    "        # Convert to pandas for time-based operations\n",
    "        df = df.compute()\n",
    "\n",
    "        # Step 2: Ensure datetime format\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "        # Step 3: Identify continuous sequences\n",
    "        df['time_diff'] = df['Datetime'].diff().dt.total_seconds()\n",
    "        df['is_continuous'] = (df['time_diff'].between(1, 120)) | (df.index == 0)\n",
    "        df['sequence_group'] = (~df['is_continuous']).cumsum()\n",
    "\n",
    "        # Step 5: Filter valid sequences\n",
    "        valid_sequences = df.groupby('sequence_group').filter(lambda x: len(x) >= window_size)\n",
    "\n",
    "        print(valid_sequences.shape)\n",
    "\n",
    "        datetime_file = \"datetime_after_seq_\" + freezer_number + \".parquet\"\n",
    "        valid_sequences[['Datetime']].to_parquet(input_path + datetime_file, index=False)\n",
    "\n",
    "        # Get the original feature names\n",
    "        feature_names = [col for col in valid_sequences.columns \n",
    "                         if col not in ['Datetime', 'time_diff', 'is_continuous', 'sequence_group']]\n",
    "\n",
    "        # Step 6: Process groups in chunks\n",
    "        def process_group(group):\n",
    "            group_windows = []\n",
    "            for start_idx in range(0, len(group) - window_size + 1):\n",
    "                window = group.iloc[start_idx:start_idx + window_size]\n",
    "                flattened_window = window.drop(\n",
    "                    ['Datetime', 'time_diff', 'is_continuous', 'sequence_group'], axis=1\n",
    "                ).values.flatten()\n",
    "                group_windows.append(flattened_window)\n",
    "            return group_windows\n",
    "\n",
    "        # Process groups in chunks to manage memory\n",
    "        chunk_size = 20\n",
    "        groups = [group for _, group in valid_sequences.groupby('sequence_group')]\n",
    "        num_chunks = (len(groups) + chunk_size - 1) // chunk_size\n",
    "\n",
    "        # Create column names for the flattened data\n",
    "        feature_columns = [f'feature_{i}_{t}' for i in range(window_size) for t in feature_names]\n",
    "\n",
    "        # Process chunks and write to temporary file\n",
    "        first_chunk = True\n",
    "\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min((chunk_idx + 1) * chunk_size, len(groups))\n",
    "            chunk_groups = groups[start_idx:end_idx]\n",
    "            \n",
    "            # Process chunk using parallel processing\n",
    "            chunk_windows = Parallel(n_jobs=num_cores)(\n",
    "                joblib_delayed(process_group)(group) for group in tqdm(\n",
    "                    chunk_groups, \n",
    "                    desc=f\"Processing Freezer {freezer_number} Chunk {chunk_idx + 1}/{num_chunks}\",\n",
    "                    unit=\"group\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Flatten the chunk results\n",
    "            chunk_windows = [window for group_windows in chunk_windows for window in group_windows]\n",
    "            \n",
    "            # Convert chunk to DataFrame with proper column names\n",
    "            chunk_df = pd.DataFrame(chunk_windows, columns=feature_columns)\n",
    "            \n",
    "            # Write chunk to temporary parquet file\n",
    "            if first_chunk:\n",
    "                chunk_df.to_parquet(temp_output_file, index=False)\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                chunk_df.to_parquet(temp_output_file, index=False, append=True)\n",
    "            \n",
    "            # Clear memory\n",
    "            del chunk_windows\n",
    "            del chunk_df\n",
    "            gc.collect()  # Trigger garbage collection\n",
    "\n",
    "        # Move the temporary file to the final location with a new name\n",
    "        shutil.move(temp_output_file, final_output_file)\n",
    "        \n",
    "        print(f\"Successfully saved processed file to: {final_output_file}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary directory\n",
    "        try:\n",
    "            shutil.rmtree(temp_dir)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Explicitly delete variables and run garbage collection\n",
    "        del df, valid_sequences, groups, feature_names, drop_columns_set_1, drop_columns_set_2\n",
    "        gc.collect()\n",
    "\n",
    "# Process all freezers\n",
    "for freezer_number in freezer_numbers:\n",
    "    print(f\"Processing freezer: {freezer_number}\")\n",
    "    process_freezer(freezer_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from joblib import Parallel, delayed as joblib_delayed\n",
    "from tqdm import tqdm\n",
    "import gc  # Import garbage collection module\n",
    "\n",
    "##### Brug ML env #####\n",
    "\n",
    "# Constants\n",
    "input_path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "output_path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "window_size = 30\n",
    "num_cores = -1  # Use all available cores\n",
    "\n",
    "# List of freezer numbers to process\n",
    "freezer_numbers = [ freezer_number ]\n",
    "\n",
    "# Function to process a single freezer\n",
    "def process_freezer(freezer_number):\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    temp_output_file = os.path.join(temp_dir, f\"processed_{freezer_number}_temp\")\n",
    "    final_output_file = os.path.join(output_path, f\"test_ready_{freezer_number}.parquet\")\n",
    "\n",
    "    try:\n",
    "        # Load data using Dask\n",
    "        df = dd.read_parquet(input_path + f'scaled_{freezer_number}_data.parquet')\n",
    "        \n",
    "        # Step 1: Drop unnecessary columns\n",
    "        drop_columns_set_1 = ['State', 'Type', 'Event']\n",
    "        drop_columns_set_2 = drop_columns_set_1 + ['main_fault']\n",
    "        if 'main_fault' in df.columns:\n",
    "            df = df.drop(columns=[col for col in drop_columns_set_2 if col in df.columns])\n",
    "        else:\n",
    "            df = df.drop(columns=[col for col in drop_columns_set_1 if col in df.columns])\n",
    "\n",
    "        # Convert to pandas for time-based operations\n",
    "        df = df.compute()\n",
    "\n",
    "        # Step 2: Ensure datetime format\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "        # Step 3: Identify continuous sequences\n",
    "        df['time_diff'] = df['Datetime'].diff().dt.total_seconds()\n",
    "        df['is_continuous'] = (df['time_diff'].between(1, 120)) | (df.index == 0)\n",
    "        df['sequence_group'] = (~df['is_continuous']).cumsum()\n",
    "\n",
    "        # Step 5: Filter valid sequences\n",
    "        valid_sequences = df.groupby('sequence_group').filter(lambda x: len(x) >= window_size)\n",
    "\n",
    "        # Get the original feature names\n",
    "        feature_names = [col for col in valid_sequences.columns \n",
    "                         if col not in ['Datetime', 'time_diff', 'is_continuous', 'sequence_group']]\n",
    "\n",
    "        def process_group(group):\n",
    "            group_windows = []\n",
    "            datetime_output = []  # Store one Datetime per flattened window\n",
    "\n",
    "            for start_idx in range(0, len(group) - window_size + 1):\n",
    "                window = group.iloc[start_idx:start_idx + window_size]\n",
    "                \n",
    "                # Save the first 'Datetime' of the window as representative for the flattened window\n",
    "                datetime_output.append(window['Datetime'].iloc[0])\n",
    "                \n",
    "                # Flatten the other columns (excluding 'Datetime')\n",
    "                flattened_window = window.drop(\n",
    "                    ['Datetime', 'time_diff', 'is_continuous', 'sequence_group'], axis=1\n",
    "                ).values.flatten()\n",
    "                group_windows.append(flattened_window)\n",
    "            \n",
    "            return group_windows, datetime_output\n",
    "\n",
    "        # Process groups in chunks to manage memory\n",
    "        chunk_size = 20\n",
    "        groups = [group for _, group in valid_sequences.groupby('sequence_group')]\n",
    "        num_chunks = (len(groups) + chunk_size - 1) // chunk_size\n",
    "\n",
    "        # Create column names for the flattened data\n",
    "        feature_columns = [f'feature_{i}_{t}' for i in range(window_size) for t in feature_names]\n",
    "\n",
    "        # Process chunks and write to temporary file\n",
    "        first_chunk = True\n",
    "\n",
    "        for chunk_idx in range(num_chunks):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min((chunk_idx + 1) * chunk_size, len(groups))\n",
    "            chunk_groups = groups[start_idx:end_idx]\n",
    "            \n",
    "            # Process chunk using parallel processing\n",
    "            chunk_results = Parallel(n_jobs=num_cores)(\n",
    "                joblib_delayed(process_group)(group) for group in tqdm(\n",
    "                    chunk_groups, \n",
    "                    desc=f\"Processing Freezer {freezer_number} Chunk {chunk_idx + 1}/{num_chunks}\",\n",
    "                    unit=\"group\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Split the results into data windows and datetime outputs\n",
    "            chunk_windows, datetime_outputs = zip(*chunk_results)\n",
    "\n",
    "            # Flatten the chunk results\n",
    "            chunk_windows = [window for group_windows in chunk_windows for window in group_windows]\n",
    "            datetime_outputs = [dt for group_datetimes in datetime_outputs for dt in group_datetimes]\n",
    "\n",
    "            # Convert chunk to DataFrame\n",
    "            chunk_df = pd.DataFrame(chunk_windows, columns=feature_columns)\n",
    "            chunk_df['Datetime'] = datetime_outputs  # Add Datetime column to the output DataFrame\n",
    "\n",
    "            # Write processed data to temporary parquet file\n",
    "            if first_chunk:\n",
    "                chunk_df.to_parquet(temp_output_file, index=False)\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                chunk_df.to_parquet(temp_output_file, index=False, append=True)\n",
    "            \n",
    "            # Clear memory\n",
    "            del chunk_windows, datetime_outputs, chunk_df\n",
    "            gc.collect()\n",
    "\n",
    "        # Move the temporary file to the final location with a new name\n",
    "        shutil.move(temp_output_file, final_output_file)\n",
    "        \n",
    "        print(f\"Successfully saved processed file to: {final_output_file}\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary directory\n",
    "        try:\n",
    "            shutil.rmtree(temp_dir)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Explicitly delete variables and run garbage collection\n",
    "        del df, valid_sequences, groups, feature_names, drop_columns_set_1, drop_columns_set_2\n",
    "        gc.collect()\n",
    "\n",
    "# Process all freezers\n",
    "for freezer_number in freezer_numbers:\n",
    "    print(f\"Processing freezer: {freezer_number}\")\n",
    "    process_freezer(freezer_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "\n",
    "#data_df = pd.read_parquet(path + \"806031_temp.parquet\")\n",
    "#data_df = pd.read_parquet(path + \"scaled_806031_data.parquet\")\n",
    "data_df = pd.read_parquet(path + \"test_ready_806031.parquet\")\n",
    "\n",
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "data_df = pd.read_parquet(path + \"test_ready_\" + freezer_number + \".parquet\")\n",
    "\n",
    "data_df = data_df.drop(['Datetime'], axis=1)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "data_df = pd.read_parquet(path + \"test_ready_\" + freezer_number + \".parquet\")\n",
    "\n",
    "data_df = data_df.drop(columns=['Datetime'])\n",
    "\n",
    "# Ensure that device is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoEncoder(hidden_units1, hidden_units2, hidden_units3, latent_features)\n",
    "model.to(device)\n",
    "\n",
    "data = data_df.to_numpy(dtype='float32')\n",
    "\n",
    "# Call the detect_anomalies function\n",
    "anomalies, reconstruction_errors = detect_anomalies_in_batches(data, model, device)\n",
    "\n",
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.75)\n",
    "plt.axvline(x=0.08, color='r', linestyle='--')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print out the indices of anomalies\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "print(f\"Detected {len(anomaly_indices):,} anomalies out of {len(data):,} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "data_df = pd.read_parquet(path + \"test_ready_\" + freezer_number + \".parquet\")\n",
    "\n",
    "data_df = data_df.drop(columns=['Datetime'])\n",
    "\n",
    "# Ensure that device is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoEncoder(hidden_units1, hidden_units2, hidden_units3, latent_features)\n",
    "model.to(device)\n",
    "\n",
    "data = data_df.to_numpy(dtype='float32')\n",
    "\n",
    "# Call the detect_anomalies function\n",
    "anomalies, reconstruction_errors = detect_anomalies_in_batches(data, model, device)\n",
    "\n",
    "data_df = pd.read_parquet(path + \"test_ready_\" + freezer_number + \".parquet\")\n",
    "\n",
    "data_df['reconstruction_error'] = reconstruction_errors\n",
    "data_df['anomaly'] = anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "data_df = pd.read_parquet(path + \"test_ready_\" + freezer_number + \".parquet\")\n",
    "\n",
    "data_df['reconstruction_error'] = reconstruction_errors\n",
    "data_df['anomaly'] = anomalies\n",
    "\n",
    "# Extract anomaly timestamps from data_df\n",
    "anomaly_timestamps = data_df['Datetime'][data_df['anomaly']].values\n",
    "\n",
    "# Downsample the data for faster plotting (e.g., every 10th point)\n",
    "downsample_rate = 10\n",
    "downsampled_data = data_df.iloc[::downsample_rate]\n",
    "downsampled_anomaly_timestamps = anomaly_timestamps[::downsample_rate]\n",
    "\n",
    "# Plot the first column over time\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(downsampled_data['Datetime'], downsampled_data['feature_0_RTD'], label='feature_0_RTD')\n",
    "\n",
    "# Add vertical red lines for anomalies\n",
    "for ts in anomaly_timestamps:\n",
    "    plt.axvline(x=ts, color='r', linestyle='--', alpha=0.1)\n",
    "\n",
    "plt.title(' Over Time with Detected Anomalies')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('RTD')\n",
    "plt.legend(loc='upper right')  # Choose a fixed location for the legend\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
