{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_parquet('cleaned_data_806016.parquet')\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Drop the 'Datetime' column (or convert it if needed)\n",
    "df.drop(columns=['Datetime', 'State', 'Type', 'Event'], inplace=True)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(df)\n",
    "\n",
    "# Step 4: Create a custom Dataset class\n",
    "class FreezerDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x, x  # For autoencoders, input and target are the same\n",
    "\n",
    "# Step 5: Create the dataset and dataloaders\n",
    "freezer_dataset = FreezerDataset(data_normalized)\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(freezer_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the number of features (i.e., columns in your dataset)\n",
    "num_features = data_normalized.shape[1]\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units, latent_features=2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=latent_features)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        z = self.encoder(x)\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        return {'z': z, 'x_hat': x_hat}\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "hidden_units = 64\n",
    "latent_features = 2\n",
    "net = AutoEncoder(hidden_units, latent_features)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "print(f\"Model is on GPU: {next(net.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize the optimizer and loss function\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss = []\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_loss = []\n",
    "    net.train()\n",
    "    \n",
    "    for x, _ in train_loader:\n",
    "        # Move input data to GPU if available\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(x)\n",
    "        x_hat = outputs['x_hat']\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(x_hat, x)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store the batch loss\n",
    "        batch_loss.append(loss.item())\n",
    "    \n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gem model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the model's state dictionary\n",
    "model_save_path = 'autoencoder_weights.pth'\n",
    "torch.save(net.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of your model\n",
    "net_loaded = AutoEncoder(hidden_units=64, latent_features=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "if cuda:\n",
    "    net_loaded = net_loaded.cuda()\n",
    "\n",
    "# Load the saved weights into the model\n",
    "model_save_path = 'autoencoder_weights.pth'\n",
    "net_loaded.load_state_dict(torch.load(model_save_path))\n",
    "print(f\"Model weights loaded from {model_save_path}\")\n",
    "\n",
    "# Set the model to evaluation mode if you are using it for inference\n",
    "net_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_anomalies(data, model, threshold=0.05):\n",
    "    model.eval()\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    \n",
    "    if cuda:\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(data_tensor)\n",
    "        x_hat = outputs['x_hat']\n",
    "        reconstruction_error = torch.mean((data_tensor - x_hat) ** 2, dim=1).cpu().numpy()\n",
    "    \n",
    "    anomalies = reconstruction_error > threshold\n",
    "    return anomalies, reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afprøve på træningsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies, reconstruction_errors = detect_anomalies(data_normalized, net)\n",
    "\n",
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.75)\n",
    "plt.axvline(x=0.05, color='r', linestyle='--')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print out the indices of anomalies\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "print(f\"Detected {len(anomaly_indices):,} anomalies out of {len(data_normalized):,} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afprøve på valid sæt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "df = pd.read_parquet('around_events_data_806016.parquet')\n",
    "#df = pd.read_parquet('around_events_data_806018.parquet')\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "df.drop(columns=['Datetime', 'State', 'Type', 'Event'], inplace=True)\n",
    "#df.drop(columns=['Datetime', 'State', 'Type', 'Event', 'main_fault'], inplace=True)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "valid_data_normalized = scaler.fit_transform(df)\n",
    "\n",
    "# Example: Detect anomalies on the training data\n",
    "anomalies, reconstruction_errors = detect_anomalies(valid_data_normalized, net)\n",
    "\n",
    "# Plot the reconstruction error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.75)\n",
    "plt.axvline(x=0.05, color='r', linestyle='--')\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print out the indices of anomalies\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "percentage_accuracy = (len(anomaly_indices) / len(valid_data_normalized)) * 100\n",
    "print(f\"Detected {len(anomaly_indices):,} anomalies out of {len(valid_data_normalized):,} samples.\")\n",
    "print(f\"Accuracy percentage: {percentage_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freezer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
