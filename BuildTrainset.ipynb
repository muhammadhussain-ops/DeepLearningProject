{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My path\n",
    "path = '/Users/claes/Documents/Mine dokumenter/Universitet/7.semester/Deep_learning/Final project/data/dataset/Revco/'\n",
    "\n",
    "#Your path\n",
    "#path = '/Users/claes/Desktop/Deep_learning/Week_6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lægge alle data fra hver fryser sammen, klar til autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load all parquet files matching the pattern\n",
    "all_files = glob.glob(path + \"cleaned_data_*.parquet\")\n",
    "#all_files = glob.glob(path + \"cleaned_data_806016.parquet\")\n",
    "\n",
    "# Step 2: Define the columns to drop (conditionally)\n",
    "drop_columns_set_1 = ['Datetime', 'State', 'Type', 'Event']\n",
    "drop_columns_set_2 = drop_columns_set_1 + ['main_fault']\n",
    "\n",
    "# Step 3: Initialize a list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Step 4: Load, preprocess, and concatenate data\n",
    "for file in all_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    # Drop columns that exist in the dataframe\n",
    "    if 'main_fault' in df.columns:\n",
    "        df.drop(columns=[col for col in drop_columns_set_2 if col in df.columns], inplace=True)\n",
    "    else:\n",
    "        df.drop(columns=[col for col in drop_columns_set_1 if col in df.columns], inplace=True)\n",
    "    \n",
    "    # Append the processed dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Step 5: Combine all dataframes into one\n",
    "combined_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Step 6: Normalize the combined dataset\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert back to a DataFrame for easier handling (optional)\n",
    "normalized_df = pd.DataFrame(data_normalized, columns=combined_df.columns)\n",
    "\n",
    "normalized_df.to_parquet(path + 'combined_cleaned_data.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Længde af combined df: 73,276,975\n"
     ]
    }
   ],
   "source": [
    "print(f\"Længde af combined df: {normalized_df.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "start_time = time.time()  # Track total time\n",
    "\n",
    "# Step 1: Load all parquet files matching the pattern\n",
    "all_files = glob.glob(path + \"cleaned_data_*.parquet\")\n",
    "\n",
    "for file in all_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 2: Define the columns to drop (conditionally)\n",
    "#drop_columns_set_1 = ['Datetime', 'State', 'Type', 'Event']\n",
    "drop_columns_set_1 = ['State', 'Type', 'Event']\n",
    "drop_columns_set_2 = drop_columns_set_1 + ['main_fault']\n",
    "\n",
    "# Step 3: Initialize a list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Step 4: Load, preprocess, and concatenate data\n",
    "for file in all_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    # Drop columns that exist in the dataframe\n",
    "    if 'main_fault' in df.columns:\n",
    "        df.drop(columns=[col for col in drop_columns_set_2 if col in df.columns], inplace=True)\n",
    "    else:\n",
    "        df.drop(columns=[col for col in drop_columns_set_1 if col in df.columns], inplace=True)\n",
    "    \n",
    "    # Append the processed dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "# Step 5: Combine all dataframes into one\n",
    "combined_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"step5\")\n",
    "print(combined_df.head(20))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Step 5 time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Step 6: Sort by Datetime and ensure it is in datetime format\n",
    "combined_df['Datetime'] = pd.to_datetime(combined_df['Datetime'])\n",
    "#combined_df.sort_values(by='Datetime', inplace=True)\n",
    "\n",
    "print(\"step6\")\n",
    "print(combined_df.head(20))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Step 6 time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Step 7: Identify continuous sequences\n",
    "# Calculate time difference in seconds\n",
    "combined_df['time_diff'] = combined_df['Datetime'].diff().dt.total_seconds()\n",
    "\n",
    "print(\"step7.1\")\n",
    "print(combined_df.head(20))\n",
    "\n",
    "# Identify the start of a new sequence\n",
    "combined_df['is_continuous'] = (combined_df['time_diff'].between(59, 61)) | (combined_df.index == 0)\n",
    "\n",
    "print(\"step7.2\")\n",
    "print(combined_df.head(20))\n",
    "\n",
    "# Assign sequence groups\n",
    "combined_df['sequence_group'] = (~combined_df['is_continuous']).cumsum()\n",
    "\n",
    "print(\"step7.3\")\n",
    "print(combined_df.head(20))\n",
    "\n",
    "# Step 8: Filter only sequences of at least 100 continuous rows\n",
    "valid_sequences = combined_df.groupby('sequence_group').filter(lambda x: len(x) >= 100)\n",
    "\n",
    "print(valid_sequences)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Step 8 time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Step 9: Create time-windowed and flattened data\n",
    "time_windows = []\n",
    "window_size = 10\n",
    "\n",
    "for _, group in valid_sequences.groupby('sequence_group'):\n",
    "    for start_idx in range(0, len(group) - window_size + 1):\n",
    "        # Extract the time window\n",
    "        window = group.iloc[start_idx:start_idx + window_size].drop(['Datetime', 'time_diff', 'is_continuous', 'sequence_group'], axis=1)\n",
    "        \n",
    "        # Flatten the window\n",
    "        flattened_window = window.values.flatten()  # Converts 2D array (100 x num_features) into 1D array\n",
    "        \n",
    "        # Append the flattened window to the list\n",
    "        time_windows.append(flattened_window)\n",
    "    print(group)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Step 9 time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(time_windows)\n",
    "\n",
    "# Convert the list of flattened windows into a DataFrame for easier handling\n",
    "flattened_df = pd.DataFrame(time_windows)\n",
    "\n",
    "# Optional: Save the flattened data for later use\n",
    "flattened_df.to_csv('flattened_time_windows.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Step 9 time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Step 10: Normalize each window\n",
    "scaler = MinMaxScaler()\n",
    "time_windows_normalized = [scaler.fit_transform(window) for window in time_windows]\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Step 10 time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Step 11: Save the prepared data (optional)\n",
    "# Convert the time-windowed data back to a DataFrame if needed\n",
    "# Note: Adjust indices or format to save them meaningfully\n",
    "prepared_data = pd.DataFrame({\n",
    "    'time_window': time_windows_normalized\n",
    "})\n",
    "\n",
    "prepared_data.to_parquet(path + 'time_windowed_cleaned_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtime_windows_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "time_windows_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
