{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If using Colab, you may need to upload `plotting.py`.           \n",
      "In the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer           \n",
      "---------------------------------------------\n",
      "No module named 'plotting'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.dark_palette(\"purple\"))\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from plotting import plot_autoencoder_stats\n",
    "except Exception as ex:\n",
    "    print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
    "          \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
    "          \\n---------------------------------------------\")\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/muhammadhussain/Desktop/Data/Revco/806016_temp.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.3, 23.4, 18.1, 28.3, 18.2, 19.8, 17.2, 21.8, -4.0, -80, 225.0,\n",
       "       0], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Flatten the 2d-array image into a vector\n",
    "flatten = lambda x: ToTensor()(x).view(28**2)\n",
    "\n",
    "# Define the train and test sets\n",
    "dset_train = MNIST(\"./\", train=True,  transform=flatten, download=True)\n",
    "dset_test  = MNIST(\"./\", train=False, transform=flatten)\n",
    "\n",
    "# The digit classes to use\n",
    "classes = [3, 7]\n",
    "\n",
    "def stratified_sampler(labels, classes):\n",
    "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "    from functools import reduce\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "    indices = torch.from_numpy(indices)\n",
    "    return SubsetRandomSampler(indices)\n",
    "\n",
    "\n",
    "# The loaders perform the actual work\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                          sampler=stratified_sampler(dset_train.targets, classes), pin_memory=cuda)\n",
    "test_loader  = DataLoader(dset_test, batch_size=batch_size, \n",
    "                          sampler=stratified_sampler(dset_test.targets, classes), pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# define size variables\n",
    "num_features = 28*28\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units, latent_features=2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # We typically employ an \"hourglass\" structure\n",
    "        # meaning that the decoder should be an encoder\n",
    "        # in reverse.\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            # bottleneck layer\n",
    "            nn.Linear(in_features=hidden_units, out_features=latent_features)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            # output layer, projecting back to image size\n",
    "            nn.Linear(in_features=hidden_units, out_features=num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = {}\n",
    "        # we don't apply an activation to the bottleneck layer\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # apply sigmoid to output to get pixel intensities between 0 and 1\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        \n",
    "        return {\n",
    "            'z': z,\n",
    "            'x_hat': x_hat\n",
    "        }\n",
    "\n",
    "\n",
    "# Choose the shape of the autoencoder\n",
    "net = AutoEncoder(hidden_units=128, latent_features=2)\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# if you want L2 regularization, then add weight_decay to SGD\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.25)\n",
    "\n",
    "# We will use pixel wise mean-squared error as our loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "x = np.random.normal(0, 1, (45, dim*dim)).astype('float32')\n",
    "\n",
    "print(net(torch.from_numpy(x)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forward pass\n",
    "# expect output size of [32, num_features]\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "\n",
    "if cuda:\n",
    "    x = x.cuda()\n",
    "\n",
    "outputs = net(x)\n",
    "print(f\"x_hat.shape = {outputs['x_hat'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_loss = []\n",
    "    net.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "        x_hat = outputs['x_hat']\n",
    "\n",
    "        # note, target is the original tensor, as we're working with auto-encoders\n",
    "        loss = loss_function(x_hat, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "    # Evaluate, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        x, y = next(iter(test_loader))\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "\n",
    "        # We save the latent variable and reconstruction for later use\n",
    "        # we will need them on the CPU to plot\n",
    "        x_hat = outputs['x_hat']\n",
    "        z = outputs['z'].cpu().numpy()\n",
    "\n",
    "        loss = loss_function(x_hat, x)\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "    \n",
    "    if epoch == 0:\n",
    "        continue\n",
    "\n",
    "    # live plotting of the trainig curves and representation\n",
    "    plot_autoencoder_stats(x=x.cpu(),\n",
    "                           x_hat=x_hat.cpu(),\n",
    "                           z=z,\n",
    "                           y=y,\n",
    "                           train_loss=train_loss,\n",
    "                           valid_loss=valid_loss,\n",
    "                           epoch=epoch,\n",
    "                           classes=classes,\n",
    "                           dimensionality_reduction_op=None)\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[neuralnetwork]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
